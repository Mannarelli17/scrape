{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import re\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bdb16",
   "metadata": {},
   "source": [
    "# weird access denied error #\n",
    "I have no idea how the Dun & Bradstreet website knows our client isn't a browser. I tried using requests with various headers and other methods but i kept getting denied. So i just decided to accept DNB's terms and created a browserwith selenium and a chrome driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469bf4b",
   "metadata": {},
   "source": [
    "Here we are just building the tools that we need going forward\n",
    "- installing and setting up the driver\n",
    "- defining the range of pages\n",
    "- defining the site that will flipped through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pages = np.arange(1, 20, 1)\n",
    "site = 'https://www.dnb.com/business-directory/company-information.beverage_manufacturing.us.html?page='\n",
    "s = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for specific sites we want, prefix to be added later to scraped data\n",
    "match = re.compile(\"/business-directory/company-profiles\")\n",
    "prefix = 'https://www.dnb.com'\n",
    "all_links = []\n",
    "for page in tqdm(pages):\n",
    "    \n",
    "    # build url\n",
    "    url = str(site) + str(page)\n",
    "    \n",
    "    # send the driver to open a browser with the site and page we want, then fetch the html\n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            break\n",
    "        except:\n",
    "            sleep(5)\n",
    "    \n",
    "    # build our soup object\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "    # retrieve and build our links\n",
    "    links = [prefix + x.get('href') for x in soup.find_all('a', href=match)]\n",
    "    \n",
    "    # append to our master link list\n",
    "    all_links += links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0a6f7",
   "metadata": {},
   "source": [
    "# retrieving data #\n",
    "Now that we have all the links, we can begin extracting data from each of the sites\n",
    "- the ```clean()``` method gets rid of random extra whitespace that occurs for some reason when scraping\n",
    "- the ```fetch_data()``` is what actually scrapes the profiles and creates a dictionary of relevent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text): return \"\" if text is None else \" \".join(text.get_text().split())\n",
    "\n",
    "def fetch_data(x):\n",
    "    # create the current dictionary\n",
    "    h = {}\n",
    "    # we tell the driver to pull up the current website and retrieve the html\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(x)\n",
    "            html = driver.page_source\n",
    "            break\n",
    "        except:\n",
    "            sleep(5)\n",
    "    \n",
    "    # make the soup object\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # here we grab readily available data that is consistently available across all companies\n",
    "    h['company name'] = [clean(soup.find('div', {'class':'company-profile-header-title'}))]\n",
    "    h['employees'] = [clean(soup.find('div', {'class':'company_data_point', 'name':'employees_all_site'}))]\n",
    "    h['annual sales'] = [clean(soup.find('div', {'class':'rev_title_number'}))]\n",
    "    \n",
    "    # here, we grab all the data names. For example, 'assets', 'liabilities', etc\n",
    "    # we do it this way because this is not consistent across profiles\n",
    "    keys = soup.find_all('th', {'class':'data_point_name'})\n",
    "    \n",
    "    # we now loop through these keys and grab there contents, storing in the dictionary\n",
    "    for key in keys:\n",
    "        sleep(1)\n",
    "        # if key = assets, the lst will be assets for 2019, 2020, 2021, etc....\n",
    "        lst = soup.select(f'tr:-soup-contains(\"{key.get_text()}\") td')\n",
    "        # we only ever deal with these years (as of now), maybe someone will have to update this one day\n",
    "        h[f'{key.get_text()} 2019'] = [clean(lst[2])]\n",
    "        h[f'{key.get_text()} 2020'] = [clean(lst[1])]\n",
    "        h[f'{key.get_text()} 2021'] = [clean(lst[0])]\n",
    "    \n",
    "    # print the name of the company so in case of error, i know who did it\n",
    "    print(h['company name'])\n",
    "    \n",
    "    # return Dictionary -> DataFrame\n",
    "    return pd.DataFrame(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964891e",
   "metadata": {},
   "source": [
    "Now all we do is iterate through our links, calling the ```fetch_data``` method, and finally concat all data to get a nice pandas DataFrame<br>From here we can write our df to an excel file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1121a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fetch_data(link) for link in tqdm(all_links)])\n",
    "writer = pd.ExcelWriter('data.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
